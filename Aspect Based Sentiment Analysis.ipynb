{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Aspect Based Sentiment Analysis using Python\n",
    "\n",
    "Link : https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-a-practical-approach-8f51029bbc4a\n",
    "Â·\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: numpy in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: nltk in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: stanfordnlp in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: transformers in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 5)) (4.34.0)\n",
      "Requirement already satisfied: sentence_transformers in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: bertopic in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 7)) (0.15.0)\n",
      "Requirement already satisfied: shap in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 8)) (0.43.0)\n",
      "Requirement already satisfied: stanza in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2022.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from nltk->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from nltk->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: click in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from nltk->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: requests in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from stanfordnlp->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from stanfordnlp->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: protobuf in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from stanfordnlp->-r requirements.txt (line 4)) (4.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (0.17.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from transformers->-r requirements.txt (line 5)) (0.14.1)\n",
      "Requirement already satisfied: torchvision in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: scipy in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.1.99)\n",
      "Requirement already satisfied: scikit-learn in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from bertopic->-r requirements.txt (line 7)) (5.17.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from bertopic->-r requirements.txt (line 7)) (0.8.33)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from bertopic->-r requirements.txt (line 7)) (0.5.4)\n",
      "Requirement already satisfied: numba in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from shap->-r requirements.txt (line 8)) (0.58.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from shap->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: slicer==0.0.7 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from shap->-r requirements.txt (line 8)) (0.0.7)\n",
      "Requirement already satisfied: emoji in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from stanza->-r requirements.txt (line 9)) (2.8.0)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from hdbscan>=0.8.29->bertopic->-r requirements.txt (line 7)) (0.29.36)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 5)) (4.3.0)\n",
      "Requirement already satisfied: fsspec in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 5)) (2023.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 7)) (8.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: sympy in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from torch>=1.0.0->stanfordnlp->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from torch>=1.0.0->stanfordnlp->-r requirements.txt (line 4)) (2.8.6)\n",
      "Requirement already satisfied: jinja2 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from torch>=1.0.0->stanfordnlp->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from umap-learn>=0.5.0->bertopic->-r requirements.txt (line 7)) (0.5.10)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from numba->shap->-r requirements.txt (line 8)) (0.41.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from numba->shap->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from requests->stanfordnlp->-r requirements.txt (line 4)) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from requests->stanfordnlp->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from requests->stanfordnlp->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from requests->stanfordnlp->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 6)) (9.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from importlib-metadata->numba->shap->-r requirements.txt (line 8)) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from jinja2->torch>=1.0.0->stanfordnlp->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vitrac/Library/Python/3.8/lib/python/site-packages (from sympy->torch>=1.0.0->stanfordnlp->-r requirements.txt (line 4)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitrac/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Natural Language Processing (NLP) libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import stanfordnlp\n",
    "\n",
    "# Text preprocessing and machine learning libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import stanza\n",
    "import string\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Topic modeling and clustering libraries\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Interpretability and explanation libraries\n",
    "import shap\n",
    "\n",
    "# Additional text processing libraries\n",
    "import nltk.stem\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "\n",
      "Default download directory: /Users/vitrac/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: /Users/vitrac/stanfordnlp_resources/en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 235M/235M [00:46<00:00, 5.01MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/vitrac/stanfordnlp_resources/en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vitrac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vitrac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vitrac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 24.8MB/s]                    \n",
      "2023-10-16 10:32:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-10-16 10:32:53 INFO: File exists: /Users/vitrac/stanza_resources/en/default.zip\n",
      "2023-10-16 10:32:56 INFO: Finished downloading models and saved to /Users/vitrac/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stanza.download('en') # This downloads the English models for the neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_index</th>\n",
       "      <th>medication</th>\n",
       "      <th>rate</th>\n",
       "      <th>comment</th>\n",
       "      <th>Treatment name</th>\n",
       "      <th>Treatment code</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Inflectra infliximab for Crohns Disease</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Insurance forced me to switch to inflectra due...</td>\n",
       "      <td>Inflectra</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Crohns Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Remicade infliximab for Rheumatoid Arthritis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife had RA from when she was a child. She ...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Rheumatoid Arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remicade infliximab for Ulcerative Colitis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This medication Treatment made me in constant ...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Ulcerative Colitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inflectra infliximab for Crohns Disease</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I have Fistulizing Crohn's, dx'd 6 yrs ago @ 3...</td>\n",
       "      <td>Inflectra</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Crohns Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Remicade infliximab for Ulcerative Colitis</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Treatment literally gave me my life back 9 yrs...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Ulcerative Colitis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_index                                    medication  rate  \\\n",
       "0           0       Inflectra infliximab for Crohns Disease   1.0   \n",
       "1           1  Remicade infliximab for Rheumatoid Arthritis   NaN   \n",
       "2           2    Remicade infliximab for Ulcerative Colitis   1.0   \n",
       "3           3       Inflectra infliximab for Crohns Disease   1.0   \n",
       "4           4    Remicade infliximab for Ulcerative Colitis  10.0   \n",
       "\n",
       "                                             comment Treatment name  \\\n",
       "0  Insurance forced me to switch to inflectra due...      Inflectra   \n",
       "1  My wife had RA from when she was a child. She ...       Remicade   \n",
       "2  This medication Treatment made me in constant ...       Remicade   \n",
       "3  I have Fistulizing Crohn's, dx'd 6 yrs ago @ 3...      Inflectra   \n",
       "4  Treatment literally gave me my life back 9 yrs...       Remicade   \n",
       "\n",
       "  Treatment code               Disease  \n",
       "0     infliximab        Crohns Disease  \n",
       "1     infliximab  Rheumatoid Arthritis  \n",
       "2     infliximab    Ulcerative Colitis  \n",
       "3     infliximab        Crohns Disease  \n",
       "4     infliximab    Ulcerative Colitis  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapt to individual path\n",
    "file_path = 'data/raw_data_healthcare.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def process_dataframe(df):\n",
    "    if 'comment' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a column named 'comment'\")\n",
    "    punctuations = string.punctuation\n",
    "    df['medication'] = df['medication'].str.replace(f\"[{re.escape(punctuations)}]\", \"\", regex=True)\n",
    "\n",
    "    # Extracting treatment name, treatment code, and disease name\n",
    "    # Adjust the pattern to match unpunctuated strings\n",
    "    pattern = r'(?P<treatment_name>.+?) (?P<treatment_code>.+?) for (?P<disease_name>.+?)( Maintenance)?$'\n",
    "\n",
    "    extracted_data = df['medication'].str.extract(pattern)\n",
    "    df['Treatment name'] = extracted_data['treatment_name']\n",
    "    df['Treatment code'] = extracted_data['treatment_code']\n",
    "    df['Disease'] = extracted_data['disease_name']\n",
    "\n",
    "    # Replacing the values in the 'comment' column\n",
    "    df['comment'] = df['comment'].replace(to_replace=extracted_data['treatment_name'].tolist(), value=\"Treatment\", regex=True)\n",
    "    df['comment'] = df['comment'].replace(to_replace=extracted_data['treatment_code'].tolist(), value=\"Treatment Code\", regex=True)\n",
    "    df['comment'] = df['comment'].replace(to_replace=extracted_data['disease_name'].tolist(), value=\"Disease\", regex=True)    \n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess the dataframe\n",
    "df = process_dataframe(df)\n",
    "\n",
    "# Display the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Implementing Preprocessing for Aspect Based Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 10:54:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 13.2MB/s]                    \n",
      "2023-10-16 10:54:21 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-10-16 10:54:21 INFO: Using device: cpu\n",
      "2023-10-16 10:54:21 INFO: Loading: tokenize\n",
      "2023-10-16 10:54:21 INFO: Loading: pos\n",
      "2023-10-16 10:54:22 INFO: Loading: lemma\n",
      "2023-10-16 10:54:22 INFO: Loading: constituency\n",
      "2023-10-16 10:54:22 INFO: Loading: depparse\n",
      "2023-10-16 10:54:22 INFO: Loading: sentiment\n",
      "2023-10-16 10:54:22 INFO: Loading: ner\n",
      "2023-10-16 10:54:23 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "def create_flagged_comment(taggedList):\n",
    "    # Pour l'Ã©tape 3\n",
    "    newwordList = []\n",
    "    flag = 0\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "            newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "            flag=1\n",
    "        else:\n",
    "            if(flag==1):\n",
    "                flag=0\n",
    "                continue\n",
    "            newwordList.append(taggedList[i][0])\n",
    "            if(i==len(taggedList)-2):\n",
    "                newwordList.append(taggedList[i+1][0])\n",
    "    finaltxt = ' '.join(word for word in newwordList)\n",
    "    return finaltxt\n",
    "\n",
    "def create_flagged_comment_get_newwordList(taggedList):\n",
    "    # Pour l'Ã©tape 3\n",
    "    newwordList = []\n",
    "    flag = 0\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "            newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "            flag=1\n",
    "        else:\n",
    "            if(flag==1):\n",
    "                flag=0\n",
    "                continue\n",
    "            newwordList.append(taggedList[i][0])\n",
    "            if(i==len(taggedList)-2):\n",
    "                newwordList.append(taggedList[i+1][0])\n",
    "    finaltxt = ' '.join(word for word in newwordList)\n",
    "    return newwordList\n",
    "\n",
    "def tokenize_and_pos_tag(comment):\n",
    "    # Pour l'Ã©tape 4\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_txt_list = nltk.word_tokenize(comment)\n",
    "    wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "    taggedList = nltk.pos_tag(wordsList)\n",
    "    return taggedList\n",
    "\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "\n",
    "def extract_dependencies(row):\n",
    "    # Fonction pour extraire les dÃ©pendances syntaxiques\n",
    "    # Pour l'Ã©tape 5\n",
    "    sentence = row['flagged_comment']\n",
    "    newwordList = row['newwordList']\n",
    "    doc = nlp(sentence)\n",
    "    dep_node = []\n",
    "    for dep_edge in doc.sentences[0].dependencies:\n",
    "        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "    for i in range(0, len(dep_node)):\n",
    "        if int(dep_node[i][1]) != 0:\n",
    "            source_index = int(dep_node[i][1]) - 1\n",
    "            if 0 <= source_index < len(newwordList):\n",
    "                dep_node[i][1] = newwordList[source_index]\n",
    "            else:\n",
    "                dep_node[i][1] = 'Out of range'\n",
    "    return dep_node\n",
    "\n",
    "def extract_features(df):\n",
    "    df = df.copy()\n",
    "    # Etape 5\n",
    "    featureList = []  # To store features for each row\n",
    "    categoriesList = []  # To store categories for each row\n",
    "    \n",
    "    for tagged_comment in df['tagged_comment']:\n",
    "        features = []\n",
    "        categories = []\n",
    "        \n",
    "        for word, pos in tagged_comment:\n",
    "            if pos in ['JJ', 'NN', 'JJR', 'NNS', 'RB']:\n",
    "                features.append([word, pos])\n",
    "                categories.append(pos)\n",
    "        \n",
    "        featureList.append(features)\n",
    "        categoriesList.append(categories)\n",
    "    \n",
    "    df['featureList'] = featureList\n",
    "    df['categoriesList'] = categoriesList\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 : Done\n",
      "Step 2 : Done\n",
      "Step 3 : Done\n",
      "Step 4 : Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting dependencies: 100%|ââââââââââ| 10/10 [00:39<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 : Done\n",
      "Step 6 : Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_index</th>\n",
       "      <th>medication</th>\n",
       "      <th>rate</th>\n",
       "      <th>comment</th>\n",
       "      <th>Treatment name</th>\n",
       "      <th>Treatment code</th>\n",
       "      <th>Disease</th>\n",
       "      <th>tokenized_comments</th>\n",
       "      <th>tagged_comment</th>\n",
       "      <th>flagged_comment</th>\n",
       "      <th>newwordList</th>\n",
       "      <th>new_sentence</th>\n",
       "      <th>dep_node</th>\n",
       "      <th>featureList</th>\n",
       "      <th>categoriesList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Inflectra infliximab for Crohns Disease</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Insurance forced me to switch to inflectra due...</td>\n",
       "      <td>Inflectra</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Crohns Disease</td>\n",
       "      <td>[insurance forced me to switch to inflectra du...</td>\n",
       "      <td>[(insurance, NN), (forced, VBD), (me, PRP), (t...</td>\n",
       "      <td>insurance forced me to switch to inflectra due...</td>\n",
       "      <td>[insurance, forced, me, to, switch, to, inflec...</td>\n",
       "      <td>[(insurance, NN), (forced, VBD), (switch, NN),...</td>\n",
       "      <td>[[insurance, forced, nsubj], [forced, 0, root]...</td>\n",
       "      <td>[[insurance, NN], [due, JJ], [cheaper, JJR], [...</td>\n",
       "      <td>[NN, JJ, JJR, NN, NN, RB, NN, NN, NN, NNS, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Remicade infliximab for Rheumatoid Arthritis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife had RA from when she was a child. She ...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Rheumatoid Arthritis</td>\n",
       "      <td>[my wife had ra from when she was a child she ...</td>\n",
       "      <td>[(my, PRP$), (wife, NN), (had, VBD), (ra, VBN)...</td>\n",
       "      <td>my wife had ra from when she was a child she h...</td>\n",
       "      <td>[my, wife, had, ra, from, when, she, was, a, c...</td>\n",
       "      <td>[(wife, NN), (ra, NN), (child, NN), (used, VBN...</td>\n",
       "      <td>[[my, wife, nmod:poss], [wife, ra, nsubj], [ha...</td>\n",
       "      <td>[[wife, NN], [child, NN], [normal, JJ], [meds,...</td>\n",
       "      <td>[NN, NN, JJ, NNS, NN, NN, NN, JJ, JJ, NN, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remicade infliximab for Ulcerative Colitis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This medication Treatment made me in constant ...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Ulcerative Colitis</td>\n",
       "      <td>[this medication treatment made me in constant...</td>\n",
       "      <td>[(this, DT), (medication, NN), (treatment, NN)...</td>\n",
       "      <td>this medicationtreatment made me in constant p...</td>\n",
       "      <td>[this, medicationtreatment, made, me, in, cons...</td>\n",
       "      <td>[(medicationtreatment, NN), (made, VBD), (cons...</td>\n",
       "      <td>[[this, medicationtreatment, det], [medication...</td>\n",
       "      <td>[[medication, NN], [treatment, NN], [constant,...</td>\n",
       "      <td>[NN, NN, JJ, NN, NN, NNS, NNS, NN, NN, RB, JJ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inflectra infliximab for Crohns Disease</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I have Fistulizing Crohn's, dx'd 6 yrs ago @ 3...</td>\n",
       "      <td>Inflectra</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Crohns Disease</td>\n",
       "      <td>[i have fistulizing crohns dxd 6 yrs ago  36yo...</td>\n",
       "      <td>[(i, NNS), (have, VBP), (fistulizing, VBG), (c...</td>\n",
       "      <td>i have fistulizing crohnsdxd 6 yrs ago 36yo 11...</td>\n",
       "      <td>[i, have, fistulizing, crohnsdxd, 6, yrs, ago,...</td>\n",
       "      <td>[(fistulizing, VBG), (crohnsdxd, NN), (6, CD),...</td>\n",
       "      <td>[[i, have, nsubj], [have, 0, root], [fistulizi...</td>\n",
       "      <td>[[i, NNS], [crohns, NN], [dxd, NN], [yrs, NN],...</td>\n",
       "      <td>[NNS, NN, NN, NN, RB, NNS, RB, JJ, NNS, NNS, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Remicade infliximab for Ulcerative Colitis</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Treatment literally gave me my life back 9 yrs...</td>\n",
       "      <td>Remicade</td>\n",
       "      <td>infliximab</td>\n",
       "      <td>Ulcerative Colitis</td>\n",
       "      <td>[treatment literally gave me my life back 9 yr...</td>\n",
       "      <td>[(treatment, NN), (literally, RB), (gave, VBD)...</td>\n",
       "      <td>treatment literally gave me my life back 9 yrs...</td>\n",
       "      <td>[treatment, literally, gave, me, my, life, bac...</td>\n",
       "      <td>[(treatment, NN), (literally, RB), (gave, VBD)...</td>\n",
       "      <td>[[treatment, gave, nsubj], [literally, gave, a...</td>\n",
       "      <td>[[treatment, NN], [literally, RB], [life, NN],...</td>\n",
       "      <td>[NN, RB, NN, RB, NN, NN, RB, JJ, NNS, NNS, NNS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_index                                    medication  rate  \\\n",
       "0           0       Inflectra infliximab for Crohns Disease   1.0   \n",
       "1           1  Remicade infliximab for Rheumatoid Arthritis   NaN   \n",
       "2           2    Remicade infliximab for Ulcerative Colitis   1.0   \n",
       "3           3       Inflectra infliximab for Crohns Disease   1.0   \n",
       "4           4    Remicade infliximab for Ulcerative Colitis  10.0   \n",
       "\n",
       "                                             comment Treatment name  \\\n",
       "0  Insurance forced me to switch to inflectra due...      Inflectra   \n",
       "1  My wife had RA from when she was a child. She ...       Remicade   \n",
       "2  This medication Treatment made me in constant ...       Remicade   \n",
       "3  I have Fistulizing Crohn's, dx'd 6 yrs ago @ 3...      Inflectra   \n",
       "4  Treatment literally gave me my life back 9 yrs...       Remicade   \n",
       "\n",
       "  Treatment code               Disease  \\\n",
       "0     infliximab        Crohns Disease   \n",
       "1     infliximab  Rheumatoid Arthritis   \n",
       "2     infliximab    Ulcerative Colitis   \n",
       "3     infliximab        Crohns Disease   \n",
       "4     infliximab    Ulcerative Colitis   \n",
       "\n",
       "                                  tokenized_comments  \\\n",
       "0  [insurance forced me to switch to inflectra du...   \n",
       "1  [my wife had ra from when she was a child she ...   \n",
       "2  [this medication treatment made me in constant...   \n",
       "3  [i have fistulizing crohns dxd 6 yrs ago  36yo...   \n",
       "4  [treatment literally gave me my life back 9 yr...   \n",
       "\n",
       "                                      tagged_comment  \\\n",
       "0  [(insurance, NN), (forced, VBD), (me, PRP), (t...   \n",
       "1  [(my, PRP$), (wife, NN), (had, VBD), (ra, VBN)...   \n",
       "2  [(this, DT), (medication, NN), (treatment, NN)...   \n",
       "3  [(i, NNS), (have, VBP), (fistulizing, VBG), (c...   \n",
       "4  [(treatment, NN), (literally, RB), (gave, VBD)...   \n",
       "\n",
       "                                     flagged_comment  \\\n",
       "0  insurance forced me to switch to inflectra due...   \n",
       "1  my wife had ra from when she was a child she h...   \n",
       "2  this medicationtreatment made me in constant p...   \n",
       "3  i have fistulizing crohnsdxd 6 yrs ago 36yo 11...   \n",
       "4  treatment literally gave me my life back 9 yrs...   \n",
       "\n",
       "                                         newwordList  \\\n",
       "0  [insurance, forced, me, to, switch, to, inflec...   \n",
       "1  [my, wife, had, ra, from, when, she, was, a, c...   \n",
       "2  [this, medicationtreatment, made, me, in, cons...   \n",
       "3  [i, have, fistulizing, crohnsdxd, 6, yrs, ago,...   \n",
       "4  [treatment, literally, gave, me, my, life, bac...   \n",
       "\n",
       "                                        new_sentence  \\\n",
       "0  [(insurance, NN), (forced, VBD), (switch, NN),...   \n",
       "1  [(wife, NN), (ra, NN), (child, NN), (used, VBN...   \n",
       "2  [(medicationtreatment, NN), (made, VBD), (cons...   \n",
       "3  [(fistulizing, VBG), (crohnsdxd, NN), (6, CD),...   \n",
       "4  [(treatment, NN), (literally, RB), (gave, VBD)...   \n",
       "\n",
       "                                            dep_node  \\\n",
       "0  [[insurance, forced, nsubj], [forced, 0, root]...   \n",
       "1  [[my, wife, nmod:poss], [wife, ra, nsubj], [ha...   \n",
       "2  [[this, medicationtreatment, det], [medication...   \n",
       "3  [[i, have, nsubj], [have, 0, root], [fistulizi...   \n",
       "4  [[treatment, gave, nsubj], [literally, gave, a...   \n",
       "\n",
       "                                         featureList  \\\n",
       "0  [[insurance, NN], [due, JJ], [cheaper, JJR], [...   \n",
       "1  [[wife, NN], [child, NN], [normal, JJ], [meds,...   \n",
       "2  [[medication, NN], [treatment, NN], [constant,...   \n",
       "3  [[i, NNS], [crohns, NN], [dxd, NN], [yrs, NN],...   \n",
       "4  [[treatment, NN], [literally, RB], [life, NN],...   \n",
       "\n",
       "                                      categoriesList  \n",
       "0  [NN, JJ, JJR, NN, NN, RB, NN, NN, NN, NNS, NN,...  \n",
       "1  [NN, NN, JJ, NNS, NN, NN, NN, JJ, JJ, NN, NN, ...  \n",
       "2  [NN, NN, JJ, NN, NN, NNS, NNS, NN, NN, RB, JJ,...  \n",
       "3  [NNS, NN, NN, NN, RB, NNS, RB, JJ, NNS, NNS, N...  \n",
       "4  [NN, RB, NN, RB, NN, NN, RB, JJ, NNS, NNS, NNS...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def preprocessing_for_ABSA(df):\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    # Ãtape 1: Tokeniser les commentaires\n",
    "    df['tokenized_comments'] = df['comment'].str.lower()\n",
    "    df['tokenized_comments'] = df['tokenized_comments'].str.replace(f\"[{re.escape(string.punctuation)}]\", \"\", regex=True)\n",
    "    df['tokenized_comments'] = df['tokenized_comments'].apply(nltk.sent_tokenize)\n",
    "    \n",
    "    print('Step 1 : Done')\n",
    "\n",
    "    # Ãtape 2: Tokenize each sentence into words, and tag each element\n",
    "    df['tagged_comment'] = df['tokenized_comments'].apply(lambda sentences: [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences])\n",
    "    df['tagged_comment'] = df['tagged_comment'].apply(lambda sentences: sentences[0])\n",
    "    print('Step 2 : Done')\n",
    "\n",
    "    # Ãtape 3: Appliquer la fonction create_flagged_comment Ã  chaque ligne\n",
    "    df['flagged_comment'] = df['tagged_comment'].apply(create_flagged_comment)\n",
    "    df['newwordList'] = df['tagged_comment'].apply(create_flagged_comment_get_newwordList)\n",
    "    print('Step 3 : Done')\n",
    "\n",
    "    # Etape 4 : Appliquer la fonction pour crÃ©er la nouvelle colonne 'new_sentence'\n",
    "    df['new_sentence'] = df['flagged_comment'].apply(tokenize_and_pos_tag)\n",
    "    print('Step 4 : Done')\n",
    "\n",
    "    # Etape 5 : Appliquer la fonction pour crÃ©er la nouvelle colonne 'dep_node'\n",
    "    tqdm.pandas(desc=\"Extracting dependencies\")\n",
    "    df['dep_node'] = df.progress_apply(extract_dependencies, axis=1)\n",
    "    print('Step 5 : Done')\n",
    "\n",
    "    # Etape 6 : Extracting the features\n",
    "    # We select only those sublists from the <dep_node> that could probably contain the features.\n",
    "    df = extract_features(df)\n",
    "    print('Step 6 : Done')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess the dataframe\n",
    "df_sample = df.iloc[:10].copy()\n",
    "df_sample = preprocessing_for_ABSA(df_sample)\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cluster :\n",
      "[['insurance', ['forced']], ['insurance', ['forced']], ['treatment', ['symptoms', 'incurred']], ['side', ['effects']], ['treatment', ['symptoms', 'incurred']], ['diet', ['healthy']], ['bloody', ['stools']], ['treatment', ['symptoms', 'incurred']], ['flare', ['ups', 'ups']], ['inflammation', ['ups', 'preventing']], ['treatment', ['symptoms', 'incurred']], ['pain', ['ribpain', 'joint', 'shoulders']], ['pain', ['ribpain', 'joint', 'shoulders']], ['pain', ['ribpain', 'joint', 'shoulders']], ['fatigue', ['horsecramping']], ['thumb', ['move']], ['fair', ['this']], ['side', ['effectsreactions']], ['doctor', ['see']], ['symptom', ['get']], ['side', ['effectsreactions']], ['treatment', ['refusing']], ['pain', ['body', 'everywhere']], ['body', ['pain']], ['pain', ['body', 'everywhere']], ['rash', ['parts']], ['body', ['pain']], ['damage', ['nerve', 'diagnose']], ['damage', ['nerve', 'diagnose']], ['everyone', ['liartill']], ['doctor', ['proves']], ['want', ['i', 'really', 'go']], ['difference', ['make']], ['remicade', ['i']], ['nothing', ['tests']], ['doctor', ['proves']], ['treatment', ['refusing']], ['doctor', ['proves']], ['time', ['wasting']], ['life', ['saved', 'my']], ['treatment', ['saved']], ['life', ['saved', 'my']], ['time', ['every']], ['insurance', ['mandatory', 'pick']], ['treatment', ['saved']], ['treatment', ['saved']], ['time', ['every']], ['trough', ['in']], ['rejection', ['to']], ['life', ['saved', 'my']], ['treatment', ['saved']], ['doctor', ['a']], ['letter', ['a']], ['time', ['every']], ['treatment', ['saved']], ['criminal', ['back']], ['insurance', ['mandatory', 'pick']], ['deserve', ['i', 'only']], ['chance', ['a']], ['treatment', ['gave', 'worked']], ['life', ['gave']], ['visit', ['first']], ['surgery', ['planning', 'done']], ['dr', ['listening']], ['treatment', ['gave', 'worked']], ['try', ['give']], ['surgery', ['planning', 'done']], ['anything', ['done', 'work']], ['anything', ['done', 'work']], ['disease', ['had']], ['treatment', ['gave', 'worked']], ['side', ['effects']], ['remission', ['i', 'effects', 'since']], ['life', ['treatment']], ['treatment', ['now', 'that', 'life']], ['disease', ['become']], ['part', ['small', 'become']], ['life', ['treatment']], ['frequency', ['increase']], ['medicine', ['started']], ['doctor', ['made']], ['deal', ['huge', 'started']], ['none', ['other']], ['dr', ['increased']], ['flare', ['major', 'took']], ['damage', ['joint']], ['treatment', ['nervous']], ['methotrexate', ['take']], ['disease', ['kidney']], ['dr', ['increased']], ['pain', ['then', 'miracle', 'swelling']], ['treatment', ['i', 'had']], ['variety', ['had']], ['crohns', ['issues', 'have']], ['fatigue', ['fistulas', 'severe']], ['side', ['effects']], ['date', ['experienced']], ['none', ['experienced']], ['treatment', ['i', 'had']], ['nothing', ['absolutely', 'did']], ['treatment', ['i', 'had']], ['effect', ['pronounced', 'had']], ['infusion', ['first']], ['hell', ['stay']], ['relapse', ['second', 'had']], ['surgery', ['having']], ['ostomy', ['wear']], ['gratitude', ['eternal']], ['treatment', ['suggested', 'life', 'has']], ['colonoscopy', ['last']], ['colon', ['looks']], ['side', ['effects']], ['none', ['experienced']], ['life', ['treatment', 'back']], ['treatment', ['suggested', 'life', 'has']], ['assistance', ['programs']], ['use', ['i', 'long']], ['medicine', ['around']], ['side', ['effects', 'liver', 'effects']], ['treatment', ['take']], ['methotrexate', ['take']], ['side', ['effects', 'liver', 'effects']], ['infusion', ['extended']], ['drug', ['great', 'doing']]]\n"
     ]
    }
   ],
   "source": [
    "def create_feature_clusters(df):\n",
    "    feature_clusters = []\n",
    "    full_feature_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        dep_node = row['dep_node']\n",
    "        features = row['featureList']\n",
    "        full_feature_list.append(features)\n",
    "        for feature, pos in features:\n",
    "            feature_cluster = []\n",
    "            for j in dep_node:\n",
    "                if (j[0] == feature or j[1] == feature) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]):\n",
    "                    if j[0] == feature:\n",
    "                        feature_cluster.append(j[1])\n",
    "                    else:\n",
    "                        feature_cluster.append(j[0])\n",
    "            if feature_cluster:\n",
    "                feature_clusters.append([feature, feature_cluster])\n",
    "    # Flatten the full_feature_list\n",
    "    flattened_feature_list = [item for sublist in full_feature_list for item in sublist]\n",
    "    return feature_clusters, flattened_feature_list\n",
    "\n",
    "feature_clusters, full_feature_list = create_feature_clusters(df_sample)\n",
    "\n",
    "finalcluster = []\n",
    "dic = {}\n",
    "for i in full_feature_list:\n",
    "    dic[i[0]] = i[1]\n",
    "for i in feature_clusters:\n",
    "    if(dic[i[0]]==\"NN\"):\n",
    "        finalcluster.append(i)\n",
    "finalcluster = [item for item in finalcluster if item[0] not in ['i', 'say', 'wife', 'child', 'japan', 'yrs']]\n",
    "finalcluster = [item for item in finalcluster if len(item[1]) < 4]\n",
    "\n",
    "print('Final cluster :')\n",
    "print(finalcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cluster :\n",
      "['insurance', ['forced']]\n",
      "['insurance', ['forced']]\n",
      "['treatment', ['symptoms', 'incurred']]\n",
      "['side', ['effects']]\n",
      "['treatment', ['symptoms', 'incurred']]\n",
      "['diet', ['healthy']]\n",
      "['bloody', ['stools']]\n",
      "['treatment', ['symptoms', 'incurred']]\n",
      "['flare', ['ups', 'ups']]\n",
      "['inflammation', ['ups', 'preventing']]\n",
      "['treatment', ['symptoms', 'incurred']]\n",
      "['pain', ['ribpain', 'joint', 'shoulders']]\n",
      "['pain', ['ribpain', 'joint', 'shoulders']]\n",
      "['pain', ['ribpain', 'joint', 'shoulders']]\n",
      "['fatigue', ['horsecramping']]\n",
      "['thumb', ['move']]\n",
      "['fair', ['this']]\n",
      "['side', ['effectsreactions']]\n",
      "['doctor', ['see']]\n",
      "['symptom', ['get']]\n",
      "['side', ['effectsreactions']]\n",
      "['treatment', ['refusing']]\n",
      "['pain', ['body', 'everywhere']]\n",
      "['body', ['pain']]\n",
      "['pain', ['body', 'everywhere']]\n",
      "['rash', ['parts']]\n",
      "['body', ['pain']]\n",
      "['damage', ['nerve', 'diagnose']]\n",
      "['damage', ['nerve', 'diagnose']]\n",
      "['everyone', ['liartill']]\n",
      "['doctor', ['proves']]\n",
      "['want', ['i', 'really', 'go']]\n",
      "['difference', ['make']]\n",
      "['remicade', ['i']]\n",
      "['nothing', ['tests']]\n",
      "['doctor', ['proves']]\n",
      "['treatment', ['refusing']]\n",
      "['doctor', ['proves']]\n",
      "['time', ['wasting']]\n",
      "['life', ['saved', 'my']]\n",
      "['treatment', ['saved']]\n",
      "['life', ['saved', 'my']]\n",
      "['time', ['every']]\n",
      "['insurance', ['mandatory', 'pick']]\n",
      "['treatment', ['saved']]\n",
      "['treatment', ['saved']]\n",
      "['time', ['every']]\n",
      "['trough', ['in']]\n",
      "['rejection', ['to']]\n",
      "['life', ['saved', 'my']]\n",
      "['treatment', ['saved']]\n",
      "['doctor', ['a']]\n",
      "['letter', ['a']]\n",
      "['time', ['every']]\n",
      "['treatment', ['saved']]\n",
      "['criminal', ['back']]\n",
      "['insurance', ['mandatory', 'pick']]\n",
      "['deserve', ['i', 'only']]\n",
      "['chance', ['a']]\n",
      "['treatment', ['gave', 'worked']]\n",
      "['life', ['gave']]\n",
      "['visit', ['first']]\n",
      "['surgery', ['planning', 'done']]\n",
      "['dr', ['listening']]\n",
      "['treatment', ['gave', 'worked']]\n",
      "['try', ['give']]\n",
      "['surgery', ['planning', 'done']]\n",
      "['anything', ['done', 'work']]\n",
      "['anything', ['done', 'work']]\n",
      "['disease', ['had']]\n",
      "['treatment', ['gave', 'worked']]\n",
      "['side', ['effects']]\n",
      "['remission', ['i', 'effects', 'since']]\n",
      "['life', ['treatment']]\n",
      "['treatment', ['now', 'that', 'life']]\n",
      "['disease', ['become']]\n",
      "['part', ['small', 'become']]\n",
      "['life', ['treatment']]\n",
      "['frequency', ['increase']]\n",
      "['medicine', ['started']]\n",
      "['doctor', ['made']]\n",
      "['deal', ['huge', 'started']]\n",
      "['none', ['other']]\n",
      "['dr', ['increased']]\n",
      "['flare', ['major', 'took']]\n",
      "['damage', ['joint']]\n",
      "['treatment', ['nervous']]\n",
      "['methotrexate', ['take']]\n",
      "['disease', ['kidney']]\n",
      "['dr', ['increased']]\n",
      "['pain', ['then', 'miracle', 'swelling']]\n",
      "['treatment', ['i', 'had']]\n",
      "['variety', ['had']]\n",
      "['crohns', ['issues', 'have']]\n",
      "['fatigue', ['fistulas', 'severe']]\n",
      "['side', ['effects']]\n",
      "['date', ['experienced']]\n",
      "['none', ['experienced']]\n",
      "['treatment', ['i', 'had']]\n",
      "['nothing', ['absolutely', 'did']]\n",
      "['treatment', ['i', 'had']]\n",
      "['effect', ['pronounced', 'had']]\n",
      "['infusion', ['first']]\n",
      "['hell', ['stay']]\n",
      "['relapse', ['second', 'had']]\n",
      "['surgery', ['having']]\n",
      "['ostomy', ['wear']]\n",
      "['gratitude', ['eternal']]\n",
      "['treatment', ['suggested', 'life', 'has']]\n",
      "['colonoscopy', ['last']]\n",
      "['colon', ['looks']]\n",
      "['side', ['effects']]\n",
      "['none', ['experienced']]\n",
      "['life', ['treatment', 'back']]\n",
      "['treatment', ['suggested', 'life', 'has']]\n",
      "['assistance', ['programs']]\n",
      "['use', ['i', 'long']]\n",
      "['medicine', ['around']]\n",
      "['side', ['effects', 'liver', 'effects']]\n",
      "['treatment', ['take']]\n",
      "['methotrexate', ['take']]\n",
      "['side', ['effects', 'liver', 'effects']]\n",
      "['infusion', ['extended']]\n",
      "['drug', ['great', 'doing']]\n"
     ]
    }
   ],
   "source": [
    "print('Final cluster :')\n",
    "for cluster in finalcluster:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
